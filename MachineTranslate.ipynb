{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import io\n",
    "import json\n",
    "import datetime\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import jieba\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "import keras\n",
    "from keras.models import Model, Sequential, load_model, model_from_json\n",
    "from keras.layers import Input, LSTM, Dense, Embedding, GRU, Bidirectional\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "en_tokenize = lambda data: [word_tokenize(sen) for sen in data]\n",
    "ch_tokenize = lambda data: [list(jieba.cut(sen)) for sen in data]\n",
    "\n",
    "def load_file(filename):\n",
    "    with open(filename, 'r', encoding = 'utf8') as f:\n",
    "        data, target = [], []\n",
    "        for row in f.readlines():\n",
    "            data.append(row.split('\\t')[0])\n",
    "            target.append(row.split('\\t')[1].replace('\\n', ''))\n",
    "    return data, target\n",
    "\n",
    "def encode_vocab(data, en):\n",
    "    if en:\n",
    "        tokenized = en_tokenize(data)\n",
    "        data_vocab = sorted(list(set( [word.lower() \\\n",
    "                                       for words in tokenized \\\n",
    "                                       for word in words] )))\n",
    "    else:\n",
    "        tokenized = ch_tokenize(data)\n",
    "        data_vocab = sorted(list(set( [word \\\n",
    "                                       for words in tokenized \\\n",
    "                                       for word in words] )))\n",
    "    \n",
    "    data_vocab = ['<PAD>', '<START>', '<END>', '<UNK>'] + data_vocab\n",
    "    vocab_code = dict(zip(data_vocab, range(len(data_vocab))))\n",
    "    vocab_size = len(vocab_code)\n",
    "    encoded = []\n",
    "    for sen in tokenized:\n",
    "        temp_sen = []\n",
    "        for word in sen:\n",
    "            if en:\n",
    "                temp_sen.append(vocab_code[word.lower()])\n",
    "            else:\n",
    "                temp_sen.append(vocab_code[word])  \n",
    "        encoded.append(temp_sen)    \n",
    " \n",
    "    return encoded, vocab_code, vocab_size\n",
    "\n",
    "def padding(data, target):\n",
    "    data_max_len = max([len(sen) for sen in data]) + 2 # add <START> and <END>\n",
    "#    data_max_len = max([len(sen) for sen in data])\n",
    "    for i, sen in enumerate(data):\n",
    "        data[i] = [1] + data[i] + [2] # add <START> and <END>\n",
    "        data[i] += [0]*(data_max_len - len(data[i]))\n",
    "    target2 = list(target)  \n",
    "    target_max_len = max([len(sen) for sen in target]) + 1 # add <START> or <END>\n",
    "#    target_max_len = max([len(sen) for sen in target])\n",
    "    for i, (sen, _) in enumerate(zip(target, target2)):\n",
    "        target[i] = [1] + sen # add <START>\n",
    "#        target[i] = sen\n",
    "        target[i] += [0]*(target_max_len - len(target[i]))\n",
    "        target2[i] = sen + [2] # add <END>  \n",
    "#        target2[i] = sen[1:] + [2]\n",
    "        target2[i] += [0]*(target_max_len - len(target2[i]))\n",
    "    \n",
    "    return data, target, target2, data_max_len, target_max_len\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" load data \"\"\"\n",
    "ori_data, ori_target = load_file('cmn.txt')\n",
    "\n",
    "\"\"\" encode vocab \"\"\"\n",
    "data, en_code, en_vocab_size = encode_vocab(ori_data, True)\n",
    "target, ch_code, ch_vocab_size = encode_vocab(ori_target, False)\n",
    "de_ch_code = dict([(v, k) for k,v in ch_code.items()])\n",
    "print('英文字典大小:', en_vocab_size, '中文字典大小:', ch_vocab_size)\n",
    "\n",
    "\"\"\" load pretrain glove weight \"\"\"\n",
    "weight_dim = 100\n",
    "with open(\"glove_model.json\", 'r') as f:\n",
    "    glove_model = json.load(f)\n",
    "\n",
    "glove_weight = [glove_model[word] \\\n",
    "                if word in glove_model.keys() \\\n",
    "                else [0.0]*weight_dim \\\n",
    "                for word in en_code]\n",
    "glove_weight = np.array(glove_weight)\n",
    "print('載入glove 100維的word embedding')\n",
    "\n",
    "\"\"\" padding \"\"\"\n",
    "#count = choose_padding_len(data)\n",
    "data, target, target2, input_size, output_size = padding(data, target)\n",
    "print('英文句子要補齊到',input_size, '個字, 中文句子要補齊到', output_size, '個字')\n",
    "data = np.array(data)\n",
    "#data = data[:, ::-1]\n",
    "target = np.array(target)\n",
    "target2 = np.array(target2)\n",
    "target2 = to_categorical(target2, num_classes=ch_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" model \"\"\"\n",
    "## constant\n",
    "batch_size = 64\n",
    "epochs = 50\n",
    "latent_dim = 256\n",
    "num_samples = 20294\n",
    "\n",
    "## encode\n",
    "encoder_inputs = Input(shape=(input_size,))\n",
    "encoder_embedding = Embedding(en_vocab_size, weight_dim, weights = [glove_weight], trainable = False)\n",
    "encoded = encoder_embedding(encoder_inputs)\n",
    "encoder = LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoded)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "## decode\n",
    "decoder_inputs = Input(shape=(output_size,))\n",
    "decoder_embedding = Embedding(ch_vocab_size, weight_dim, trainable = True)\n",
    "decoded = decoder_embedding(decoder_inputs)\n",
    "decoder = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder(decoded, initial_state=encoder_states)\n",
    "\n",
    "## fully connected\n",
    "dense = Dense(ch_vocab_size, activation='softmax')\n",
    "decoder_outputs1 = dense(decoder_outputs)\n",
    "\n",
    "## training model\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs1)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit([data, target], target2,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_split=0.2,\n",
    "          verbose = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" inference model \"\"\"\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoded = decoder_embedding(decoder_inputs)\n",
    "decoder_outputs, state_h, state_c = decoder(decoded, \n",
    "                                            initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs1 = dense(decoder_outputs)\n",
    "decoder_model = Model([decoder_inputs] + decoder_states_inputs,\n",
    "                      [decoder_outputs1] + decoder_states)\n",
    "\n",
    "t = str(datetime.datetime.now())[:16]\n",
    "encoder_model.save(t + '_encoder_model.h5')\n",
    "decoder_model.save(t + '_decoder_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\" test output \"\"\"\n",
    "json_string = model.to_json(indent = 4)\n",
    "test_index = [4077, 2122, 3335, 1464, 8956, 7168, 3490, 4495, 5100, 119]\n",
    "outputs = ''\n",
    "\n",
    "for index in test_index:\n",
    "    stop_condition = False\n",
    "    decoded_sentence = []\n",
    "    \n",
    "    test_data = data[index-1].reshape(1,-1)\n",
    "    print('Input sentence:', ori_data[index-1])\n",
    "    outputs += 'Input sentence: ' + ori_data[index-1] + '\\n'\n",
    "    \n",
    "    states = encoder_model.predict(test_data)    \n",
    "    target_seq = np.zeros((1, output_size))\n",
    "    target_seq[0, 0] = 1 # first word is <START>\n",
    "    \n",
    "    word_n = 0\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states)\n",
    "#        states = [h, c]\n",
    "        # Sample a token\n",
    "        ch_index = np.argmax(output_tokens[0, word_n, :])\n",
    "        ch_word = de_ch_code[ch_index]\n",
    "        decoded_sentence.append(ch_word)\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if ch_word == '<END>' or len(decoded_sentence)+1 > output_size:\n",
    "            break\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        word_n += 1\n",
    "#        target_seq = np.zeros((1, output_size))\n",
    "        target_seq[0, word_n] = ch_index # first word is ch_word\n",
    "\n",
    "        # Update states\n",
    "        \n",
    "        \n",
    "    print('Output sentence:', ' '.join(decoded_sentence))\n",
    "    print('---')\n",
    "    outputs += 'Output sentence: ' + ' '.join(decoded_sentence) + \"\\n---\\n\"\n",
    "    \n",
    "with open('outputs.txt', 'w') as f:\n",
    "    f.write(outputs)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
